deTector: a Topology-aware Monitoring System for Data Center NetworksYanghua PengThe University of Hong KongJi YangChuan WuXi'an Jiaotong UniversityThe University of Hong KongChuanxiong GuoChengchen HuZongpeng LiMicrosoft ResearchXi'an Jiaotong UniversityUniversity of CalgaryAbstractTroubleshooting network performance issues is a challenging task especially in large-scale data center networks. This paper presents deTector, a network monitoring system that is able to detect and localize network failures (manifested mainly by packet losses) accurately in near real time while minimizing the monitoring overhead. deTector achieves this goal by tightly coupling detection and localization and carefully selecting probe paths so that packet losses can be localized only according to end-to-end observations without the help of additional tools (e.g., tracert). In particular, we quantify the desirable properties of the matrix of probe paths, i.e., coverage and identifiability, and leverage an efficient greedy algorithm with a good approximation ratio and fast speed to select probe paths. We also propose a loss localization method according to loss patterns in a data center network. Our algorithm analysis, experimental evaluation on a Fattree testbed and supplementary large-scale simulation validate the scalability, feasibility and effectiveness of deTector.high quality of service (OoS) for users (e.g., no more than a few minutes of downtime per month [21]) and to increase revenue for operators.Rapid failure recovery is not possible without a good network monitoring system. There have been a number of systems proposed in the past few years [36, 26, 37,48]. Several limitations still exist in these systems that prohibit fast failure detection and localization.First, existing monitoring systems may fail to detect one type of failures or another. Traditional passive monitoring approaches, such as querying the device counter via SNMP or retrieving information via device CLI when users have perceived some issues, can detect clean failures such as link down, line card malfunctions. However, gray failures may occur, i.e., faults not detected or ignored by the device, or malfunctioning not properly reported by the device due to some bugs [37]. Active monitoring systems (e.g., Pingmesh [26], NetNORAD [37])can detect such failures by sending end-to-end probes, but they may fail to capture failures that cause low rate losses, due to ECMP in data center (§2).1 IntroductionA variety of services are hosted in large-scale data centers today, e.g., search engines, social networks and file sharing. To support these services with high quality, data center networks (DCNs) are carefully designed to efficiently connect thousands of network devices together,e.g., a 64-ary Fattree [9] DCN has more than 60,000 servers and 5,000 switches. However, due to the large network scale, frequent upgrades and management complexity, failures in DCNs are the norm rather than the exception [21], such as routing misconfigurations, link flaps, etc. Among these failures, those leading to userperceived performance issues (e.g., packet losses, latency spikes) are among the first priority to be detected and eliminated promptly [27, 26, 21], in order to maintainSecond, probe systems such as Pingmesh and NetNORAD inject probes between each pair of servers without selection, which may introduce too much bandwidth overhead. In addition, they typically treat the whole DCN as a black box, and hence require many probes to cover all parallel paths between any server pair with high probability.Third, failures in the network can be reported in these active monitoring systems, but the exact failure locations cannot be pinpointed automatically. The network operator typically learns a suspected source-destination server pair once packet loss happens. Then she/he needs to resort to additional tools such as tracert to verify the issue and locate the faulty spot. However, it may be difficult to play back the issues due to transient failures. Hence this diagnosis approach (i.e., separation of detection and localization) may take several hours or even days to pinpoint the fault spot [21], yet ideally the failures should be repaired as fast as possible before users complain.A desirable monitoring system in a DCN should meet three objectives: exhaustive failure detection (i.e., detecting as many types of losses as possible), low overhead and real-time failure localization. In this paper, we seek to investigate the following question: if we are aware of the network topology of a DCN, can we design a much better network monitoring system that achieves all these goals? Our answer is deTector, a topology-aware network monitoring system that we design, implement and evaluate following the three design objectives. The secret weapon of deTector is a carefully designed probe matrix (84), which achieves good link coverage, identifiability and evenness. deTector is designed to detect and localize network failures manifested by user-perceptible performance problems such as packet losses and latency spikes in large-scale data centers. We mainly focus on packet loss in this paper, but deTector can also handle latency issues by treating a round trip time (RTT) larger than a threshold as a packet loss. Throughout the paper, we use "failure localization", "fault localization" and "loss localization" interchangeably. Specifically, we make the following contributions in developing deTector.tor achieves greater than 98% accuracy in failure localization with a less than 1% false positive ratio for most failures in large-scale DCNs ($6). We have open sourced deTector [6].2 MotivationDCNs are usually multi-stage Clos networks with multiple paths between commodity servers for load balancing and fault tolerance [9, 22, 26, 45]. Each DCN has its favorable routing protocols for path selection. For example, in a Fattree topology [9] and a VL2 topology [22], the shortest paths between any two ToRs are typically used in practice [30]. We describe how existing monitoring systems fall short in achieving the three design objectives. Table 1 shows detailed comparison among deTector and the existing systems.> As compared to the existing active monitoring systems adopting end-to-end probes (e.g., Pingmesh [26], NetNORAD [37]), we treat each switch instead of the whole network as a blackbox, i.e., our system requires the knowledge of the network topology and routing protocols in a DCN (i.e., topology-aware) and we use source routing to control the probing path. In order to achieve real-time failure localization, we couple detection and localization closely and only rely on end-to-end measurements to localize failures without the help of other tools(e.g., fbtracert [3]). To make it possible, we quantify several desirable properties of probe matrix (e.g., identifiability) and propose a greedy algorithm to minimize probe cost. To address the scalability issue in DCNs, we apply several optimization heuristics and exploit characteristics of the DCN topology to accelerate path computation ($4).The passive approach stores packet statistics on switch counters, which are polled from SNMP or CLI periodically. In Fig. 1, if link AB is down, the switch counters will show a lot of packet losses. However, if the failure is a gray failure rather than link down, it may go undetected. For example, when silent packet drops occur, the switch do not show any packet drop hints (e.g., syslog errors) due to various reasons (e.g., ASIC deficit), and hence SNMP data may not be fully trustworthy [26]. Furthermore, switches counters can be noisy, such that problems identified by this approach may or may not lead to end-to-end delay or loss perceived by users.> We modify a failure localization algorithm based on packet loss characteristics in large-scale data centers. Compared to the existing algorithms, our algorithm runs within seconds and achieves higher accuracy and lower false positive rate ($5).Pingmesh and NetNORAD adopt an end-to-end probing approach to measure network latency and packet loss. Pingmesh selects probe paths by constructing two complete graphs within a DCN: one includes all servers under the same ToR switch (i.e., the switch in the edge layer in Fig. 1) and the other spans all ToR switches. NetNORAD is similar to Pingmesh but places pingers in a few pods instead of all servers. Their approaches simplify the design but bring quite significant overhead ($6). Although gray failures can be captured, it is difficult to detect failures causing low rate losses (e.g., 1%) of a link, when ECMP is adopted in the DCN: there are many paths beCore> We implement and evaluate our system on a 4ary Fattree testbed built with 20 switches. The experiments show that deTector is practically deployable and can accurately localize failures in near real time with less probe overhead, e.g., for 98% accuracy, deTector requires 3.9x and 1.9x times fewer probes than Pingmesh and NetNORAD while localizing failures 30 seconds in advance without the use of other loss localization tools. Our supplementary simulation further shows that deTecAggregationEdge ToRsPod 1Pod 2Pod3Pod 4Figure 1: A 4-radix Fattree topology: failure on link AB can be detected by sending probes from s1 to s3. Table 1: Comparison among deTector and existing representative monitoring systemsGray failures Low rate loss Failure localization SNMP/CLITransient failuresTimelinessOverheadNesYesPingmesh [26]NetNORAD[3]Yes YesNoNo, need Netbouncerminutes minutes minutesswitch resourcesmany probesNo, need fbtracertNomany probes, switch CPUdeTectorYesYesYesYesnear real-timeminimal probestween a pair of servers, low-rate losses on a particular link may not affect much the overall end-to-end loss rate between the two servers.The exact location of losses cannot be pinpointed using Pingmesh or NetNORAD, since they do not know which paths the probes take (e.g., due to ECMP). Therefore, other tools such as Netbouncer [4] and fbtracert [3] are needed, which send additional probes to play back the losses. These post-alarm tools may fail to pinpoint transient failures, those caused by transient bit errors, nonatomic rule updates or network upgrade (e.g., a transient inconsistency between the link configuration and routing information [21]). To pinpoint such failures, close coupling of detection and localization is required, so that losses are localized only according to detection data, instead of additional probes after detection alarms. Such coupling further enables near real-time fault localization.3 System Designping configuration (§6.1). The probe paths from a ToR switch to different destinations are distributed among pinglists of pingers under the ToR switch, with each path distributed to at least 2 pingers for fault tolerance. In this way, in case that one pinger is down, other pingers in the same rack can still probe the paths, avoiding any large drop in link coverage. To detect failure on links connecting servers and the respective ToRs, pingers are also responsible for probing other servers under the same ToR. The number of probe paths for each pinger is no more than a hundred even for a large DCN (§4.4). The probe packets are sent over UDP. Though TCP is used to carry most traffic in a DCN, the DCN does not differentiate TCP and UDP traffic (e.g., the forwarding behavior) in the vast majority of cases [37, 26], and hence UDP probes can also manifest network performance. When a pinger detects a probe loss, it confirms the loss pattern by sending two probe packets of the same content additionally.3.1 ArchitecturedeTector includes four loosely coupled components: aResponder. The responder is a lightweight module running on all servers. Upon receiving a probe packet, the responder echoes it back. A responder does not retain any states and all probing results are logged by pingers.controller, a diagnoser, pingers and responders, as depicted in Fig. 2.ControllerPinglistsDiagnoser. Each pinger records packet loss information and sends it to the diagnoser for loss localization. These logs are saved into a database for real-time analysis and later queries. The diagnoser runs the PLL algorithm ($5) to pinpoint packet losses and estimates the loss rates of suspected links.Pinger_UDP ProbesResponderFor the controller and the diagnoser to be fault-tolerant and scalable, we can use existing solutions (e.g., Software Load-Balancer [41, 26]).ResultsDiagnoser3.2 Workflow OverviewFigure 2: System architecturedeTector works in three steps in cycles: path computation, network probing and loss localization.Controller. The logical controller periodically constructs the probe matrix indicating the paths for sending probes (see $4 for details). We mainly focus on failure localization on links inter-connecting switches, as the fault on a link connecting a server with a ToR switch can be easily identified as discussed in the next paragraph. The probe matrix indicates paths between ToRs. Since we do not rely on ToRs with ping capability, probes are sent by 2-4 selected servers (pingers) under each ToR.Path computation. At the beginning of each cycle, the controller reads the data center topology and server health from data center management service (e.g., [31]), and selects the minimal number of probe paths (84). The controller then selects pingers in each ToR, constructs and dispatches the pinglists to them.Pinger. Each pinger receives the pinglist from the controller, which contains server targets, probe format andNetwork probing. Next, probe packets are sent along the specified paths across the DCN. Since data center usually adopts ECMP for load balancing, we have to use source routing to control the path traveled by each probe packet, which can be implemented using various methods.1 A general and feasible solution is to employ packet encapsulation and decapsulation to create end-toend tunnels, though it may cause encapsulating packets twice in virtualized networks created by VXLAN [1] or NVGRE [2]. Take the Fattree network in Fig. 1 as an example: fixing a core switch, there is only one path between two inter-pod servers; we can use IP-in-IP encapsulation to wrap the probe on a server; after the packet arrives at the core switch, the outer header is removed and the packet is routed to the real destination. Such a source routing mechanism incurs little overhead on servers and core switches.1 2 311 3 12 13 23P111Ο111R=P2د→ R'1Ο1111P3100001011Figure 3: Extend routing matrix with virtual linksProblem 1 Given a DCN routing matrix R, select a set of paths to construct a probe matrix P, such that P simultaneously (I) minimizes the number of paths, and achieves (2) a-coverage and (3) β-identifiability.Loss localization. The probe loss measurements are aggregated and analyzed by our loss localization algorithm ($5) on the diagnoser. We pinpoint the faulty links, estimate the loss rates, and send alerts to the network operator for further action (e.g., examining switch logs).Minimizing the number of probe paths is desirable for minimizing network bandwidth consumption and analysis overhead, such that we may finish probing and diagnosing the entire DCN in merely a few minutes. Under the same probing bandwidth budget, it allows each pinger to probe the same set of paths more frequently.4 Probe Matrix DesignThe main limitation of existing monitoring systems is that the probe path selection is far from optimum, such that not enough useful information can be collected and additional probes are needed to reproduce losses for localization. In this section, we elaborate how we carefully select probe paths to overcome such a limitation.a-coverage requires that each link is covered by at least o paths in the probe matrix. Covering a link multiple times brings higher statistical accuracy for loss detection, as well as better resilience to pinger failures (since a link is more likely to be covered by probes from multiple pingers).4.1 ProblemConsider a data center network graph G = (V, E), where V is the set of switches and E is the set of links. R is them × n routing matrix defined byRi,j =(1if link jis on path iโootherwiseβ-identifiability states that the simultaneous failures of any (no more than) β links in the DCN can be localized correctly. For the routing matrix in Fig. 3, suppose we select pi and p2 to constitute the probe matrix, i.e., the probe matrix contains the first two rows of R. If 2 or more links fail simultaneously, the faulty links cannot be correctly identified, as the observation from the end is the same, i.e., packet losses are observed on both paths. On the other hand, if only one link is faulty, the bad link can be identified effectively: losses are observed on both paths, p1, or p2 if link 1, 2, or 3 is faulty, respectively. Therefore, the probe matrix achieves 1-identifiability, but not 2 or higher identifiability. Better identifiability contributes to higher accuracy of loss localization.where m is the number of paths and n = El is the number of links. The possible paths and the routing matrix are decided by the routing protocols employed in the data center, e.g., ECMP is typically used to exploit k2 /4 parallel paths between any two ToRs in a k-ary Fattree. Fig. 3 gives a routing matrix R with 3 paths and 3 links. Note that each link in a DCN is typically bi-directional. Once we select a path from server s1 to server s2 and send a probe, the reverse path from s2 to s1 is automatically selected, since the response packet can probe faults along the reverse direction. When we identify that link AB has failed, it implies that the failure may lie in either direction of the link, switch A, or switch B.We find that Problem 1 is NP-hard for general DCNs as the Minimum Set Cover Problem is a special case of the problem. We hence resort to an approximation algorithm to compute the probing path, which is at the heart of deTector.4.2 PMC AlgorithmWe extend a well-known greedy algorithm [13] for constructing a probe matrix achieving 1-identifiability to one achieving β-identifiability, as well as o-coverage using a minimal number of probe paths.Source routing protocols have been designed in some DCNs like BCube [24] and DCell [25]; [30, 32] introduce other solutions for explicit path control.In a probe matrix, a link belongs to a set of paths. To achieve 1-identifiability, the path sets of different links should all be different, so that losses can be observed on a particular set of paths to identify the faulty link. Recall that the set of links in our DCN is E. Once we select a path from the set of all feasible paths decided by the routing matrix based on some criterion, it splits E into two subsets E1 and E2, containing links on the selected path and the other links, respectively. If we do not observe any packet loss on this path, it implies that all links in Ei are good; otherwise, there must be at least one bad link in E1. Similarly, we select another path to further split E, and E2 into smaller subsets, and repeat this procedure. Eventually if we can obtain subsets each containing only one link, then the probe matrix constructed using the selected paths achieves 1-identifiability (since the set of paths traversing each link is unique); otherwise, there does not exist a 1-identifiable probe matrix in the DCN. Throughout the process, if we always select a path whose links are present in the largest number of link sets to further split the link sets as much as possible, we will end up with the minimal number of paths needed.Algorithm 1 Probe Matrix Construction (PMC) AlgorithmRequire: R, α, β1: Initialize w, score to 0, setnum to 1, selpaths to 02: R' ←LINKOR(R, β)3: paths ← all paths in R', physlinks ← E4: while (setnum # |E| || physlinks +0) && paths +0 do5:6:for path E paths do update score[path] according to (1)7:8:9:path ← argminpath' Epaths Score[path']selpaths ← selpathsU {path}paths ← paths/{path}10:for physlink on path do11:w[physlink] ← w[physlink]+1To achieve β-identifiability, we expand the DCN graph G with "virtual links". A virtual link is a combination of multiple physical links, and the set of paths a virtual link belongs to can be computed by "OR"-ing together the paths including the individual links [13]. For the example in Fig. 3, the original routing matrix R is extended to R' with three additional virtual links l12, 13 and l23 added; the column corresponding to the virtual link l12 can be computed by "OR"-ing the two columns corresponding to links l and 2. For B-identifiability. Σ C(lEl, i) virtual links should be added in the DCNif w[physlink] > a thenphyslinks ← physlinks/{physlink}14:update setnum as the total number of link sets after split by path15: return probe matrix constructed by paths in selpaths (retaining only physical links on the paths)in the above procedure. We strive to achieve better evenness among the links while guaranteeing a-coverage, by always selecting a path with the lowest score.2SSβgraph (routing matrix), corresponding to all combinations of 2 to β links in the original graph. Then we can run the above algorithm for constructing 1-identifiable matrix based on the new routing matrix, and the resulting probe matrix achieves β-identifiability.The probe matrix does not achieve even path coverage among the links yet. For example, for a 1-identifiable probe matrix constructed on a 64-ary Fattree, the gap between the maximal and minimal numbers of probing paths passing through any two links can be as large as 188. To achieve better evenness (i.e., spreading paths and thus probe overhead evenly among the physical links), we introduce a link weight w[link), denoting the number of paths that the link resides on, and ensure that it is no smaller than a for any physical link. We also define a score for each (extended) path, i.e., the path includes virtual links from the extended routing matrix R':score(path) =w[link] — # of link sets on pathlinkepath(1)Our Probe Matrix Construction algorithm, PMC, is summarized in Alg. 1. We first reduce the problem of constructing a β-identifiable matrix to one constructing 1-identifiable matrix, by adding virtual links to the original routing matrix of the DCN graph (line 2, where LINKOR denotes the method for extending routing matrix discussed above). Then in each iteration we update the score of each (extended) path (lines 5-6) and select a path which has the minimal score among all candidate paths (lines 7-8). We remove the selected path from the candidate path set (line 9), and update the weight of physical links (w[physlink]) on the selected path (lines 10-11) and the total number of link sets that the already selected paths can split into (line 14, which corresponds to the procedure discussed in the second paragraph of this subsection). If the number of paths that cover one (physical) link exceeds a, we remove the link from the set of all links (line 12-13). The loop stops when the probe matrix achieves α-coverage (i.e., the set physlinks is empty) and β-identifiability (i.e., the number of link sets split equals the number of links), or there are no more candidate paths (i.e., the set paths is empty).Here the link sets are the split link sets produced by the procedure above. We say that a link set is on a path if the. link set contains at least one (physical or virtual) link of the path. Thus, a lower score indicates that the links on the path are not covered much by paths already selected and/or more link sets can be split if the path is selectedTheorem 1 The PMC algorithm achieves (1-) approximation of the optimum in terms of the total number of probe paths selected, where e is natural logarithm. We can prove Theorem 1 by showing that the score of a path set is monotone, submodular and non-negative.

The detailed proof is in the technical report [7]. In practice, the PMC algorithm performs much better than the (1-) ≈ 0.63 approximation ratio (§4.4). The issue of this algorithm, however, is the computation time. The time complexity of the algorithm is O(m2), where m is the number of paths, since in the worst case we may update the scores of all paths in each iteration and end up with selecting all paths. In a 64-radix Fattree, there are about 4.3 x 10° desirable paths among ToRs. As we will see in $4.4, the algorithm is still too slow for any data center at a reasonable scale, and we adopt a number of optimizations to further speed it up.4.3 Algorithm SpeedupTo speed up the PMC algorithm, we apply several optimizations based on the following three observations.Observation 1Problem 1 can be divided into a series of subproblems.We can construct a bipartite graph according to the routing matrix: one partition corresponds to paths and the other consists of links; an edge exists between a path node and a link node if the link is on the path. We observe that if the routing matrix can be partitioned into sets of paths with no links in common, then the problem can be divided into independent subproblems. For example, in Fig. 1, paths traversing the red link have no link overlapping with paths traversing the blue link. Therefore, the bipartite graph can typically be divided into connected subgraphs and each subgraph represents a smaller routing matrix and hence a subproblem. Finding connected subgraphs can be done in linear time by traversing the bipartite graph once. Then the PMC algorithm can be applied to the subproblems in parallel.Observation 2The score ofeach path is non-decreasing over all iterations.It can be proved that the score of a path is non-decreasing (Appendix A in [7]). Inspired by the CELF algorithm for outbreak detection in networks [38], we adopt a strategy called lazy update which defers the update of a path score as much as possible even though we know the score is outdated. Specifically, we maintain a min-heap for all paths with scores as the keys and only update the score of a path when the path is at the top of the heap. After score update, if the path still stays at the top of the heap,i.e., the path has the minimal score among all available paths, we will select the path as a probe path, even though some path scores have yet to be updated. The correctness of this heuristic is guaranteed by submodularity of the score of a path set: the marginal gain provided by a path selected in the current iteration can not be larger than that provided by the path in the previous iteration. Observation 3 The DCN topology is typically symmetric.Due to symmetry, when a path is selected, all its topologically isomorphic paths can be selected. For example in Fig. 1, if the dashed green path spanning Pod 1 and Pod 2 is selected, then the dashed purple path spanning Pod 3 and Pod 4 may be a good choice too. This helps us reduce the scale of the problem since the routing matrix R can be reduced to a smaller matrix by excluding paths that are topologically isomorphic to other paths. For example, if the green path is in the matrix, we do not need to include the purple path. For this purpose, we first need to compute the symmetric components in a DCN graph. There are many fast algorithms available for symmetry discovery [17, 15], e.g., O2 [15] can finish computation within 5 seconds for a Fattree(100) DCN, and we only need to precompute it once for a DCN.4.4 PerformanceWe run our PMC algorithm on a Dell PowerEdge R430 rack server with 10 Intel Xeon E5-2650 CPUs and 48GB memory, to test its running time and number of paths selected. We compare results on three well-known DCNs,Fattree [9], VL2 [22] and BCube [24].2